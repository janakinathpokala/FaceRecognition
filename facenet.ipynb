{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Face Recognition using ConvNet Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Architecture Used:\n",
    "\n",
    "* CNN\n",
    "* inception model\n",
    "* FaceNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell imports all the libraries required for the running the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import os,os.path\n",
    "import mtcnn\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_available_gpus():\n",
    "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
    "\n",
    "    # Returns\n",
    "        A list of available GPU devices.\n",
    "    \"\"\"\n",
    "    #global _LOCAL_DEVICES\n",
    "    if tfback._LOCAL_DEVICES is None:\n",
    "        devices = tf.config.list_logical_devices()\n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend.tensorflow_backend as tfback\n",
    "tfback._get_available_gpus = _get_available_gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. By understanding the architecture of the FaceNet and using Openface github rep i tried to develop the architecture similar in    the FaceNet using inception model.\n",
    "\n",
    "2. Since training a deep CNN model requires large dataset and high computational power for training i have used pretrained model    weights of the FaceNet from Openface and loaded into my model.\n",
    "\n",
    "3. Below cell creates a model and initializes the model with pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the below cell we see our model input shape and output shape\n",
    "* input= (m,3,96,96)\n",
    "* output= (m,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input_1:0' shape=(None, 3, 96, 96) dtype=float32>]\n",
      "[<tf.Tensor 'lambda_1/l2_normalize:0' shape=(None, 128) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(FRmodel.inputs)\n",
    "print(FRmodel.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the preprocessing step i have tried different ways for input to the pretrained model:\n",
    "\n",
    "1. Used just the faces of different people but with little higher extra pixels and i was able to get the results but the         threshold ie. minimum distance for recognition has to be set to be around 2 which is little high.\n",
    " \n",
    "2. Used more closer faces with not much extra pixels and this time i was able to get results with minimum distance for recognition has to be set around 1 with it ok.\n",
    "\n",
    "3. Explored different face detection and finally MTCNN did much better than all the things. So used MTCNN for face detection in pre-processing stage in pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Running the  below 2 cells will do:\n",
    "\n",
    "1. opens the webcam of the laptop.\n",
    "2. takes around 10 images of the person in front of webcam and detect the faces from images using MTCNN and stores in the present working directory images folder. This serves as a database file which we will use later for 128 embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faces_to_database_for_embedding():\n",
    "    vc = cv2.VideoCapture(0)\n",
    "    if vc.isOpened(): # try to get the first frame\n",
    "        rval, frame = vc.read()\n",
    "    else:\n",
    "        rval = False\n",
    "    count=0\n",
    "    while rval:\n",
    "        cv2.imshow(\"Face Recognition\", frame)\n",
    "        rval, frame = vc.read()\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pix=np.asarray(image)\n",
    "        K.set_image_data_format('channels_last')\n",
    "        detect= mtcnn.MTCNN()\n",
    "        result_faces=detect.detect_faces(pix)\n",
    "        for fa in result_faces:\n",
    "            (x,y,width,height)=fa['box']\n",
    "            x1=abs(x)\n",
    "            y1=abs(y)\n",
    "            x2=x1+width\n",
    "            y2=y1+height\n",
    "            face=pix[y1:y2,x1:x2]\n",
    "            image = PIL.Image.fromarray(face)\n",
    "            image = image.resize((96,96),PIL.Image.ANTIALIAS)\n",
    "            image.save('images/janardhan'+str(count+1)+'.jpg')\n",
    "            count+=1\n",
    "        key = cv2.waitKey(20)\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "        elif count>=10:\n",
    "            break\n",
    "    cv2.destroyWindow(\"Face Recognition\")\n",
    "    vc.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_faces_to_database_for_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running the below 2 cells will:\n",
    "\n",
    "1. It creates a dictionary of 128 embedding vectors for each image file in the database using our model which has pretrained initial weights.\n",
    "\n",
    "2. This embedding vectors are latter used to find the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_embedding():\n",
    "    dict={}\n",
    "    for f in os.listdir('images'):\n",
    "        impath='images\\\\'+f\n",
    "        dict[f.split('.')[0]]=img_to_encoding(impath,FRmodel)\n",
    "    return dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_faces=create_image_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Below function is use to the similarity between images:\n",
    "\n",
    "1. This function takes the image path which has to be recognized and conver that into 128 embedding vector.\n",
    "2. Using for loop it compares with each of the 128 embedding vector of databse image and see if there is any similarity.\n",
    "3. If the minimum distance is greater than 0.7  we consider them as not similar and hence not identified.\n",
    "4. If the minimum distance is less than 0.7 then it return the distance and name of the person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model):\n",
    "   \n",
    "    \n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "    \n",
    "\n",
    "    min_dist = 100\n",
    "    \n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        dist = np.linalg.norm(db_enc-encoding)\n",
    "        if dist<min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        #print(\"Not in the database.\")\n",
    "        identity=None\n",
    "    #else:\n",
    "        #print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Running the below 2 cells will do:\n",
    "\n",
    "1. Open the webcam for input image for Face recgonition\n",
    "2. It perform MTCNN algorithm on image and detect the face in image\n",
    "3. From this we have the (x1,y1),(x2,y2) of the face.\n",
    "4. Now we perform who_is_it to see if the identified face has any similar face in database.\n",
    "5. if it identifies any image we get the name else None.\n",
    "6. We draw the box around the face using the coordinates we obtained and write the name of similar identity or None is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def recognize_input_face(database):\n",
    "    vc = cv2.VideoCapture(0)\n",
    "    if vc.isOpened(): # try to get the first frame\n",
    "        rval, frame = vc.read()\n",
    "    else:\n",
    "        rval = False\n",
    "    while rval:\n",
    "        cv2.imshow(\"Face Recognition\", frame)\n",
    "        rval, frame = vc.read()\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pix=np.asarray(image)\n",
    "        K.set_image_data_format('channels_last')\n",
    "        detect= mtcnn.MTCNN()\n",
    "        result_faces=detect.detect_faces(pix)\n",
    "        K.set_image_data_format('channels_first')\n",
    "        for fa in result_faces:\n",
    "            (x,y,width,height)=fa['box']\n",
    "            x1=abs(x)\n",
    "            y1=abs(y)\n",
    "            x2=x1+width\n",
    "            y2=y1+height\n",
    "            face=pix[y1:y2,x1:x2]\n",
    "            image = PIL.Image.fromarray(face)\n",
    "            image = image.resize((96,96),PIL.Image.ANTIALIAS)\n",
    "            image.save('temp.jpg') \n",
    "            _,name=who_is_it('temp.jpg',database,FRmodel)\n",
    "            image =cv2.rectangle(frame,(x1, y1),(x2, y2),(255,255,255),2)\n",
    "            if name is not None:\n",
    "                image=cv2.putText(image,re.split(r'(\\d+)',name)[0],(x1+5,y1-5),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "            else:\n",
    "                image=cv2.putText(image,str(name),(x1+5,y1-5),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "        cv2.imshow(\"Face Recognition\", image)\n",
    "        key = cv2.waitKey(100)\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "    cv2.destroyWindow(\"Face Recognition\")\n",
    "    vc.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_input_face(DB_faces)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. FaceNet: A Unified Embedding for Face Recognition and Clustering (https://arxiv.org/pdf/1503.03832.pdf)\n",
    "\n",
    "2. DeepFace:Closing the Gap to Human-Level Performance in FaceVerification(https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf)\n",
    "\n",
    "3. Pre-trained OpenFace mode from Keras-OpenFace(Pre trained weights can be found here) (https://github.com/iwantooxxoox/Keras-OpenFace)Â \n",
    "\n",
    "4. https://github.com/davidsandberg/facenet\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
